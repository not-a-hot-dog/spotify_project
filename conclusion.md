# Conclusion
<!-- KR putting down some ideas here if helpful to you -->

## Comparison of Models and Performance
<!-- How did individual modeling approches compare to each other and score? You can include the three sets of model metrics charts here side by side in a table -->

|             | Collaborative Filtering | K Means | Naive Bayes |
|-------------|-------------------------|---------|-------------|
| Hit Rate    | 0.136                   | 0.195   | 0.191       |
| R Precision | 0.152                   | 0.133   | 0.145       |
| R Precision | 0.313                   | 0.219   | 0.116       |
| NDCG        | 0.152                   | 0.154   | 0.163       |
| NDCG        | 0.249                   | 0.203   | 0.234       |

## Discussion
<!-- How did we do overall? Are the scores comparable to other models? Is there a chance that some of our predictions were better than what was in the original playlists? -->

## Future Developments
<!-- Could we incorporate additional data, like Genre, into a future iteration -->
<!-- Would we want to adapt the code for scalability and performance -->
<!-- Could we extend these approaches to another task, e.g., predicting courses you may like based on your course reviews and others' course reviews, as well as you major and other courses in your Crimson Cart? -->
